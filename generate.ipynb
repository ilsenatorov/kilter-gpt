{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.models.gpt import GPTModel\n",
    "from src.utils import Tokenizer\n",
    "import torch\n",
    "import pandas as pd\n",
    "from src.utils import Plotter\n",
    "import wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download the model weights and init the model\n",
    "\n",
    "# run = wandb.init(name=\"test\")\n",
    "# artifact = run.use_artifact(\"ilsenatorov/kilter-gpt/model-17j5ahs6:v0\", type=\"model\")\n",
    "# artifact_dir = artifact.download()\n",
    "\n",
    "model = GPTModel.load_from_checkpoint(\"artifacts/model-17j5ahs6:v0/model.ckpt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# init the plotter and tokenizer\n",
    "df = pd.read_csv(\"data/raw/climbs.csv\")\n",
    "plotter = Plotter()\n",
    "tokenizer = Tokenizer.from_df(df, angle=True, grade=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a prompt and plot the holds from the prompt\n",
    "hold_prompt = \"p1135r12p1395r14\"\n",
    "prompts = [\n",
    "    (hold_prompt, 30, \"5a\"),\n",
    "    (hold_prompt, 30, \"6a\"),\n",
    "    (hold_prompt, 30, \"7a\"),\n",
    "    (hold_prompt, 30, \"8a\"),\n",
    "]\n",
    "plotter.plot_climb(hold_prompt, True)\n",
    "# tokenize, remove EOS token, pad left\n",
    "tokenized_prompts = torch.stack(\n",
    "    [\n",
    "        tokenizer.encode(\n",
    "            *x,\n",
    "            eos=False,\n",
    "            pad=model.config.context_len,\n",
    "        )\n",
    "        for x in prompts\n",
    "    ]\n",
    ").to(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# higher temperature -> more randomness. 0.2 is a good value for this model\n",
    "TEMP = 0.1\n",
    "generated = model.generate(tokenized_prompts, 50, temperature=TEMP)\n",
    "fig, axs = plt.subplots(1, 4, figsize=(15, 4))  # 2 rows, 2 columns\n",
    "\n",
    "for i, z in enumerate(tokenizer.decode_batch(generated)):\n",
    "    frames, angle, grade = tokenizer.clean(z)\n",
    "\n",
    "    axs[i].imshow(plotter.plot_climb(frames))\n",
    "    axs[i].set_title(f\"{grade} @ {angle[1:]}Â°\")\n",
    "    axs[i].axis(\"off\")  # Remove axis ticks and labels\n",
    "\n",
    "fig.suptitle(f\"Temp: {TEMP}\")\n",
    "plt.tight_layout()  # Adjust spacing for better layout\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if the model is good, save it as torchscript\n",
    "script = model.to_torchscript(file_path=\"good_model.pt\", method=\"script\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "",
   "language": "python",
   "name": ""
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
